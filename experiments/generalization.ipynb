{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How good are the models at generalization to other datasets? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"LS\" # LS or IS (latent/image)\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "utils_path = os.path.abspath(os.path.join('../../'))\n",
    "if utils_path not in sys.path:\n",
    "    sys.path.append(utils_path)\n",
    "from utils.notebookutils import SimaseUSLatentDataset,SimaseUSVideoDataset, SiameseNetwork, model_forward_to_corrcoeff,model_forward_to_pred, model_forward_to_bin_pred, model_forward_to_corr_coeff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load models and dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set testing dataset seed to 0\n",
      "Set testing dataset seed to 0\n",
      "Set testing dataset seed to 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/ideadata/ed52egek/conda/latecho/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/vol/ideadata/ed52egek/conda/latecho/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "\n",
    "normalization =lambda x: (x  - x.min())/(x.max() - x.min()) * 2 - 1  # should be -1 to 1 due to way we trained the model\n",
    "\n",
    "#datasets\n",
    "if mode == \"LS\": \n",
    "    ds_test_dynamic = SimaseUSLatentDataset(phase=\"testing\", transform=normalization, latents_csv=\"/vol/ideadata/at70emic/projects/TMI23/data/diffusion/dynamic/FileList.csv\", training_latents_base_path=\"/vol/ideadata/at70emic/projects/TMI23/data/diffusion/dynamic/Latents\", in_memory=False, generator_seed=0)\n",
    "    ds_test_psax = SimaseUSLatentDataset(phase=\"testing\", transform=normalization, latents_csv= \"/vol/ideadata/at70emic/projects/TMI23/data/diffusion/PSAX/FileList.csv\", training_latents_base_path= \"/vol/ideadata/at70emic/projects/TMI23/data/diffusion/PSAX/Latents\", in_memory=False, generator_seed=0)\n",
    "    ds_test_a4c = SimaseUSLatentDataset(phase=\"testing\", transform=normalization, latents_csv=\"/vol/ideadata/at70emic/projects/TMI23/data/diffusion/A4C/FileList.csv\", training_latents_base_path= \"/vol/ideadata/at70emic/projects/TMI23/data/diffusion/A4C/Latents\", in_memory=False, generator_seed=0)\n",
    "else: \n",
    "\n",
    "    ds_test_dynamic = SimaseUSVideoDataset(phase=\"testing\", transform=normalization, latents_csv=\"/vol/ideadata/at70emic/datasets/EchoNet-Dynamic/FileList.csv\", training_latents_base_path= \"/vol/ideadata/at70emic/datasets/EchoNet-Dynamic/Videos\", in_memory=False, generator_seed=0)\n",
    "    ds_test_psax = SimaseUSVideoDataset(phase=\"testing\", transform=normalization, latents_csv= \"/vol/ideadata/at70emic/datasets/Echonet-Peds/PSAX/processed/FileList.csv\", training_latents_base_path= \"/vol/ideadata/at70emic/datasets/Echonet-Peds/PSAX/processed/Videos\", in_memory=False, generator_seed=0)\n",
    "    ds_test_a4c = SimaseUSVideoDataset(phase=\"testing\", transform=normalization, latents_csv=\"/vol/ideadata/at70emic/datasets/Echonet-Peds/A4C/processed/FileList.csv\", training_latents_base_path= \"/vol/ideadata/at70emic/datasets/Echonet-Peds/A4C/processed/Videos\", in_memory=False, generator_seed=0)\n",
    "\n",
    "datasets = {\"d\": ds_test_dynamic, \"p\": ds_test_psax, \"a\": ds_test_a4c}\n",
    "ds_name_to_name = {\"d\": \"Dynamic\", \"p\": \"PSAX\", \"a\": \"A4C\"}\n",
    "\n",
    "#load models\n",
    "models = {\"a\": None, \"d\": None, \"p\": None}\n",
    "for model_name, model_ending in zip([\"a\", \"d\", \"p\"], [\"a4c\", \"Dynamic\", \"psax\"]): \n",
    "    model_basepath = f\"/vol/ideadata/ed52egek/pycharm/privatis_us/archive/{model_ending}{mode}Best\"\n",
    "    with open(os.path.join(model_basepath, \"config.json\")) as config:\n",
    "        config = config.read()\n",
    "\n",
    "    # parse config\n",
    "    config = json.loads(config)\n",
    "    net = SiameseNetwork(network=config['siamese_architecture'], in_channels=config['n_channels'], n_features=config['n_features'])\n",
    "    net.eval()\n",
    "    net = net.cuda()\n",
    "    best_name = [x for x in os.listdir(model_basepath) if x.endswith(\"best_network.pth\")][0]\n",
    "    net.load_state_dict(torch.load(os.path.join(model_basepath, best_name)))\n",
    "    models[model_name] = net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for IS\n",
      "Results for Train Dynamic -- Test: Dynamic:      $0.76 \\pm 0.12$\n",
      "Results for Train A4C -- Test: Dynamic:      $0.72 \\pm 0.16$\n",
      "Results for Train PSAX -- Test: Dynamic:      $0.73 \\pm 0.12$\n",
      "Results for Train Dynamic -- Test: A4C:      $0.71 \\pm 0.15$\n",
      "Results for Train A4C -- Test: A4C:      $0.71 \\pm 0.14$\n",
      "Results for Train PSAX -- Test: A4C:      $0.59 \\pm 0.21$\n",
      "Results for Train Dynamic -- Test: PSAX:      $0.71 \\pm 0.15$\n",
      "Results for Train A4C -- Test: PSAX:      $0.70 \\pm 0.15$\n",
      "Results for Train PSAX -- Test: PSAX:      $0.68 \\pm 0.13$\n"
     ]
    }
   ],
   "source": [
    "print(f\"Results for {mode}\")\n",
    "for ds_name in [\"d\", \"a\", \"p\"]:\n",
    "    for model_name in [\"d\", \"a\", \"p\"]:\n",
    "        corr_preds = []\n",
    "        for i in range(len(datasets[ds_name])):\n",
    "            pred = model_forward_to_corrcoeff(models[model_name], datasets[ds_name][i)\n",
    "            corr_preds.append(pred.flatten())\n",
    "\n",
    "        corr_preds = torch.cat(corr_preds)\n",
    "        mean = float(corr_preds.mean())\n",
    "        std = float(corr_preds.std())\n",
    "\n",
    "        # Output LaTeX formatted string\n",
    "        print(f\"Results for Train {ds_name_to_name[model_name]} -- Test: {ds_name_to_name[ds_name]}:      \" + f\"${mean:.2f} \\\\pm {std:.2f}$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Train Dynamic -- Test: Dynamic:      1.000\n",
      "Results for Train A4C -- Test: Dynamic:      0.994\n",
      "Results for Train PSAX -- Test: Dynamic:      0.996\n",
      "Results for Train Dynamic -- Test: A4C:      0.995\n",
      "Results for Train A4C -- Test: A4C:      1.000\n",
      "Results for Train PSAX -- Test: A4C:      0.984\n",
      "Results for Train Dynamic -- Test: PSAX:      0.997\n",
      "Results for Train A4C -- Test: PSAX:      1.000\n",
      "Results for Train PSAX -- Test: PSAX:      0.995\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "for ds_name in [\"d\", \"a\", \"p\"]:\n",
    "    for model_name in [\"d\", \"a\", \"p\"]:\n",
    "        # set same seat for each run to ensure comparability\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "\n",
    "        dataset = datasets[ds_name]\n",
    "        dataset.reset_generator()\n",
    "        model = models[model_name]\n",
    "        for i in range(len(dataset)):\n",
    "            if dataset.generator.uniform() < 0.5: \n",
    "                y = 0 \n",
    "                vid_a = dataset[i]\n",
    "                vid_b = torch.clone(dataset.get_vid((i + dataset.generator.integers(low=1, high=len(dataset))) % len(dataset))) # random different vid\n",
    "            else: \n",
    "                y = 1\n",
    "                vid_a = dataset[i] \n",
    "                vid_b = dataset[i] \n",
    "\n",
    "            frame_a = dataset.generator.integers(len(vid_a))\n",
    "            frame_b = (frame_a + dataset.generator.integers(low=1, high=len(vid_b))) % len(vid_b)\n",
    "\n",
    "            y_pred.append(model_forward_to_pred(model, vid_a[frame_a].unsqueeze(dim=0), vid_b[frame_b].unsqueeze(dim=0)))\n",
    "            y_true.append(y)\n",
    "            #if i == 2: \n",
    "            #   break\n",
    "\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        y_true = np.stack(y_true)\n",
    "\n",
    "        auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "        print(f\"Results for Train {ds_name_to_name[model_name]} -- Test: {ds_name_to_name[ds_name]}:      \" + f\"{auc:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_forward_to_abs(model, input_a, input_b, bs=256): \n",
    "    # two single frame videos --> corr coeff according to model \n",
    "    coeffs = []\n",
    "    with torch.no_grad():\n",
    "        for i in np.arange(0, len(input_a), bs):\n",
    "            batch_a = input_a[i:i+bs].cuda()\n",
    "            batch_b = input_b[i:i+bs].cuda()\n",
    "            feature_a = model.forward_once(batch_a)\n",
    "            feature_b = model.forward_once(batch_b)\n",
    "        coeffs.append(-1 *  torch.abs(feature_a - feature_b).mean().cpu())\n",
    "    coeffs = torch.stack(coeffs)\n",
    "    return coeffs\n",
    "\n",
    "def model_forward_to_mse(model, input_a, input_b, bs=256): \n",
    "    # two single frame videos --> corr coeff according to model \n",
    "    coeffs = []\n",
    "    with torch.no_grad():\n",
    "        for i in np.arange(0, len(input_a), bs):\n",
    "            batch_a = input_a[i:i+bs].cuda()\n",
    "            batch_b = input_b[i:i+bs].cuda()\n",
    "            feature_a = model.forward_once(batch_a)\n",
    "            feature_b = model.forward_once(batch_b)\n",
    "        coeffs.append(-1 * ((feature_a - feature_b)**2).mean().cpu())\n",
    "    coeffs = torch.stack(coeffs)\n",
    "    return coeffs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PREDICTION_MODE: mae\n",
      "Results for Train Dynamic -- Test: Dynamic:      0.995\n",
      "Results for Train A4C -- Test: Dynamic:      0.978\n",
      "Results for Train PSAX -- Test: Dynamic:      0.986\n",
      "Results for Train Dynamic -- Test: A4C:      0.998\n",
      "Results for Train A4C -- Test: A4C:      0.986\n",
      "Results for Train PSAX -- Test: A4C:      0.989\n",
      "Results for Train Dynamic -- Test: PSAX:      0.998\n",
      "Results for Train A4C -- Test: PSAX:      0.996\n",
      "Results for Train PSAX -- Test: PSAX:      0.999\n",
      "Unweighted mean of all datasets for prediction mode mae: 0.9916936256523824\n",
      " 0.995 & 0.978 & 0.986 & 0.998 & 0.986 & 0.989 & 0.998 & 0.996 & 0.999 & 0.992\n",
      "================================================================================\n",
      "PREDICTION_MODE: abs\n",
      "Results for Train Dynamic -- Test: Dynamic:      0.996\n",
      "Results for Train A4C -- Test: Dynamic:      0.987\n",
      "Results for Train PSAX -- Test: Dynamic:      0.990\n",
      "Results for Train Dynamic -- Test: A4C:      0.998\n",
      "Results for Train A4C -- Test: A4C:      0.986\n",
      "Results for Train PSAX -- Test: A4C:      0.990\n",
      "Results for Train Dynamic -- Test: PSAX:      0.999\n",
      "Results for Train A4C -- Test: PSAX:      0.998\n",
      "Results for Train PSAX -- Test: PSAX:      0.998\n",
      "Unweighted mean of all datasets for prediction mode abs: 0.9936038638336918\n",
      " 0.996 & 0.987 & 0.990 & 0.998 & 0.986 & 0.990 & 0.999 & 0.998 & 0.998 & 0.994\n",
      "================================================================================\n",
      "PREDICTION_MODE: fcout\n",
      "Results for Train Dynamic -- Test: Dynamic:      1.000\n",
      "Results for Train A4C -- Test: Dynamic:      0.994\n",
      "Results for Train PSAX -- Test: Dynamic:      0.996\n",
      "Results for Train Dynamic -- Test: A4C:      0.995\n",
      "Results for Train A4C -- Test: A4C:      1.000\n",
      "Results for Train PSAX -- Test: A4C:      0.984\n",
      "Results for Train Dynamic -- Test: PSAX:      0.997\n",
      "Results for Train A4C -- Test: PSAX:      1.000\n",
      "Results for Train PSAX -- Test: PSAX:      0.995\n",
      "Unweighted mean of all datasets for prediction mode fcout: 0.9955887508126666\n",
      " 1.000 & 0.994 & 0.996 & 0.995 & 1.000 & 0.984 & 0.997 & 1.000 & 0.995 & 0.996\n",
      "================================================================================\n",
      "PREDICTION_MODE: corr\n",
      "Results for Train Dynamic -- Test: Dynamic:      0.996\n",
      "Results for Train A4C -- Test: Dynamic:      0.978\n",
      "Results for Train PSAX -- Test: Dynamic:      0.970\n",
      "Results for Train Dynamic -- Test: A4C:      0.998\n",
      "Results for Train A4C -- Test: A4C:      0.984\n",
      "Results for Train PSAX -- Test: A4C:      0.984\n",
      "Results for Train Dynamic -- Test: PSAX:      0.999\n",
      "Results for Train A4C -- Test: PSAX:      0.997\n",
      "Results for Train PSAX -- Test: PSAX:      0.993\n",
      "Unweighted mean of all datasets for prediction mode corr: 0.9887318400658942\n",
      " 0.996 & 0.978 & 0.970 & 0.998 & 0.984 & 0.984 & 0.999 & 0.997 & 0.993 & 0.989\n"
     ]
    }
   ],
   "source": [
    "for PREDICTION_MODE in [\"mse\", \"abs\", \"fcout\", \"corr\"]: \n",
    "    aucs = []\n",
    "    print(\"=\"*80)\n",
    "    print(f\"PREDICTION_MODE: {PREDICTION_MODE}\")\n",
    "    latex = \"\"\n",
    "    for ds_name in [\"d\", \"a\", \"p\"]:\n",
    "        for model_name in [\"d\", \"a\", \"p\"]:\n",
    "            # set same seat for each run to ensure comparability\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "\n",
    "            dataset = datasets[ds_name]\n",
    "            dataset.reset_generator()\n",
    "            model = models[model_name]\n",
    "            for i in range(len(dataset)):\n",
    "\n",
    "                if dataset.generator.uniform() < 0.5: \n",
    "                    y = 0 \n",
    "                    vid_a = dataset[i]\n",
    "                    vid_b = torch.clone(dataset.get_vid((i + dataset.generator.integers(low=1, high=len(dataset))) % len(dataset))) # random different vid\n",
    "                else: \n",
    "                    y = 1\n",
    "                    vid_a = dataset[i] \n",
    "                    vid_b = dataset[i] \n",
    "\n",
    "                frame_a = dataset.generator.integers(len(vid_a))\n",
    "                frame_b = (frame_a + dataset.generator.integers(low=1, high=len(vid_b))) % len(vid_b)\n",
    "                if PREDICTION_MODE == \"corr\":\n",
    "                    y_pred_cur = model_forward_to_corr_coeff(model, vid_a[frame_a].unsqueeze(dim=0), vid_b[frame_b].unsqueeze(dim=0))\n",
    "                elif PREDICTION_MODE == \"fcout\": \n",
    "                    y_pred_cur = model_forward_to_pred(model, vid_a[frame_a].unsqueeze(dim=0), vid_b[frame_b].unsqueeze(dim=0))\n",
    "                elif PREDICTION_MODE == \"abs\": \n",
    "                    y_pred_cur = model_forward_to_abs(model, vid_a[frame_a].unsqueeze(dim=0), vid_b[frame_b].unsqueeze(dim=0))\n",
    "                elif PREDICTION_MODE == \"mse\": \n",
    "                    y_pred_cur = model_forward_to_mse(model, vid_a[frame_a].unsqueeze(dim=0), vid_b[frame_b].unsqueeze(dim=0))\n",
    "                else: \n",
    "                    raise ValueError(\"Unknown prediction type\")\n",
    "\n",
    "                y_pred.append(y_pred_cur)\n",
    "                y_true.append(y)\n",
    "                #if i == 2: \n",
    "                #   break\n",
    "\n",
    "            y_pred = np.concatenate(y_pred)\n",
    "            y_true = np.stack(y_true)\n",
    "\n",
    "            auc = metrics.roc_auc_score(y_true, y_pred)\n",
    "            aucs.append(auc)\n",
    "            latex += f\" {auc:.3f} &\"\n",
    "            print(f\"Results for Train {ds_name_to_name[model_name]} -- Test: {ds_name_to_name[ds_name]}:      \" + f\"{auc:.3f}\")\n",
    "    mean = float(np.stack(aucs).mean())\n",
    "    latex += f\" {mean:.3f}\"\n",
    "    print(f\"Unweighted mean of all datasets for prediction mode {PREDICTION_MODE}: {mean}\")\n",
    "    print(latex)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I took the auc values of the original training runs to stay consistent. These are even better (perfect for dynamic, psax)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
